{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation and Bias-Variance decomposition\n",
    "## Cross-Validation\n",
    "Implementing 4-fold cross-validation below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import load_data\n",
    "\n",
    "# load dataset\n",
    "x, y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_indices = build_k_indices(y, 4, 1)\n",
    "print(k_indices,k_indices)\n",
    "print((np.delete(k_indices, 3, 0)).flatten())\n",
    "print( k_indices[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from costs import compute_mse\n",
    "from ridge_regression import ridge_regression\n",
    "from build_polynomial import build_poly\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # ***************************************************\n",
    "    # get k'th subgroup in test, others in train\n",
    "    # ***************************************************\n",
    "    idx_tr = (np.delete(k_indices, 3, 0)).flatten()\n",
    "    idx_te = k_indices[k]\n",
    "    x_tr, y_tr = x[idx_tr], y[idx_tr]\n",
    "    x_te, y_te = x[idx_te], y[idx_te]\n",
    "    # ***************************************************\n",
    "    # form data with polynomial degree\n",
    "    # ***************************************************\n",
    "    x_poly_tr = build_poly(x_tr, degree)   \n",
    "    x_poly_te = build_poly(x_te, degree)\n",
    "    # ***************************************************\n",
    "    # ridge regression:\n",
    "    # ***************************************************\n",
    "    loss_tr, ws = ridge_regression(y_tr, x_poly_tr, lambda_)\n",
    "    # ***************************************************\n",
    "    # calculate the loss for train and test data\n",
    "    # ***************************************************\n",
    "    loss_te = compute_mse(y_te, x_poly_te, ws)\n",
    "    return loss_tr, loss_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots import cross_validation_visualization\n",
    "\n",
    "def cross_validation_demo():\n",
    "    seed = 1\n",
    "    degree = 7\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-4, 0, 30)\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    var_tr = []\n",
    "    var_te = []\n",
    "    # ***************************************************\n",
    "    # cross validation:\n",
    "    # ***************************************************  \n",
    "    for lambda_ in lambdas:\n",
    "        k_indices = build_k_indices(y, k_fold, seed)\n",
    "        loss_ave_tr = []; loss_ave_te = []\n",
    "        for k in range(k_fold):\n",
    "            loss_tr, loss_te = cross_validation(y, x, k_indices, k, lambda_, degree)\n",
    "            loss_ave_tr.append(np.sqrt(2*loss_tr)); loss_ave_te.append(np.sqrt(2*loss_te))\n",
    "        rmse_tr.append(np.mean(loss_ave_tr))\n",
    "        var_tr.append(np.var(loss_ave_tr))\n",
    "        rmse_te.append(np.mean(loss_ave_te))\n",
    "        var_te.append(np.var(loss_ave_te))\n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "    plt.figure()\n",
    "    cross_validation_visualization(lambdas, var_tr, var_te)\n",
    "    plt.title('Variation of Cross Validation')\n",
    "    plt.ylabel('var')\n",
    "\n",
    "cross_validation_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "degrees = np.linspace(2,10,dtype=int)\n",
    "k_fold = 4\n",
    "lambda_ = 10e-2\n",
    "# split data in k fold\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "# define lists to store the loss of training data and test data\n",
    "rmse_tr = []\n",
    "rmse_te = []\n",
    "var_tr = []\n",
    "var_te = []\n",
    "# ***************************************************\n",
    "# cross validation:\n",
    "# ***************************************************  \n",
    "for degree in degrees:\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    loss_ave_tr = []; loss_ave_te = []\n",
    "    for k in range(k_fold):\n",
    "        loss_tr, loss_te = cross_validation(y, x, k_indices, k, lambda_, degree)\n",
    "        loss_ave_tr.append(np.sqrt(2*loss_tr)); loss_ave_te.append(np.sqrt(2*loss_te))\n",
    "    rmse_tr.append(np.mean(loss_ave_tr))\n",
    "    var_tr.append(np.var(loss_ave_tr))\n",
    "    rmse_te.append(np.mean(loss_ave_te))\n",
    "    var_te.append(np.var(loss_ave_te))\n",
    "plt.figure()\n",
    "plt.plot(degrees, rmse_tr, marker=\".\", color='b', label='train error')\n",
    "plt.plot(degrees, rmse_te, marker=\".\", color='r', label='test error')\n",
    "plt.title('Best polynomial degree is '+str(degrees[np.argmin(rmse_te)]))\n",
    "plt.xlabel('rmse')\n",
    "plt.ylabel('polynomial degree')\n",
    "plt.figure()\n",
    "plt.plot(degrees, var_tr, marker=\".\", color='b', label='train error')\n",
    "plt.plot(degrees, var_te, marker=\".\", color='r', label='test error')\n",
    "plt.title('Most stable polynomial degree model is '+str(degrees[np.argmin(var_te)]))\n",
    "plt.xlabel('polynomial degree')\n",
    "plt.ylabel('var')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Decomposition\n",
    "Visualize bias-variance trade-off by implementing the function `bias_variance_demo()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from least_squares import least_squares\n",
    "from split_data import split_data\n",
    "from plots import bias_variance_decomposition_visualization\n",
    "\n",
    "def bias_variance_demo():\n",
    "    \"\"\"The entry.\"\"\"\n",
    "    # define parameters\n",
    "    seeds = range(100)\n",
    "    num_data = 10000\n",
    "    ratio_train = 0.005\n",
    "    degrees = range(1, 10)\n",
    "    \n",
    "    # define list to store the variable\n",
    "    rmse_tr = np.empty((len(seeds), len(degrees)))\n",
    "    rmse_te = np.empty((len(seeds), len(degrees)))\n",
    "    \n",
    "    for index_seed, seed in enumerate(seeds):\n",
    "        np.random.seed(index_seed)\n",
    "        x = np.linspace(0.1, 2 * np.pi, num_data)\n",
    "        y = np.sin(x) + 0.3 * np.random.randn(num_data).T\n",
    "        # ***************************************************\n",
    "        # split data with a specific seed\n",
    "        # ***************************************************\n",
    "        x_tr, y_tr, x_te, y_te = split_data(x, y, ratio_train, index_seed)\n",
    "        # ***************************************************\n",
    "        # bias_variance_decomposition\n",
    "        # ***************************************************\n",
    "        for degree in degrees:\n",
    "            x_poly_tr = build_poly(x_tr, degree)   \n",
    "            x_poly_te = build_poly(x_te, degree)\n",
    "            mse, ws = least_squares(y_tr, x_poly_tr)\n",
    "            rmse_tr[index_seed,degree-1] = np.sqrt(2*mse)\n",
    "            rmse_te[index_seed,degree-1] = np.sqrt(2*compute_mse(y_te, x_poly_te, ws))\n",
    "    bias_variance_decomposition_visualization(degrees, rmse_tr, rmse_te)\n",
    "\n",
    "bias_variance_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.reshape(range(10),(2,5))\n",
    "print(a)\n",
    "print(a.T)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
