{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 832,
     "status": "ok",
     "timestamp": 1602683126000,
     "user": {
      "displayName": "Davide Rosso",
      "photoUrl": "",
      "userId": "18358579221259765961"
     },
     "user_tz": -120
    },
    "id": "hE0xtnAn4pja"
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First : exploring the data\n",
    "\n",
    "We'll need to have a look at what the data is, how it is distributed for the different features, and start to get an intuition about what methods might work better for analysis and prediction later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJIVugOC4pje"
   },
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "executionInfo": {
     "elapsed": 920,
     "status": "error",
     "timestamp": 1602683129008,
     "user": {
      "displayName": "Davide Rosso",
      "photoUrl": "",
      "userId": "18358579221259765961"
     },
     "user_tz": -120
    },
    "id": "6ywLVx4a4pje",
    "outputId": "6c3dc9cc-eaed-41d6-b186-c5c48d1c9066"
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PoPlUo5WU-CC"
   },
   "outputs": [],
   "source": [
    "# remove samples with error values\n",
    "idx_c = np.all(tX!=-999, axis=1)\n",
    "y_c = y[idx_c]\n",
    "tX_c = tX[idx_c]\n",
    "# regularize\n",
    "mean = np.mean(tX_c, axis=0)\n",
    "std = np.std(tX_c, axis=0)\n",
    "tX_c = (tX_c-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HGLf-JCXU-CE"
   },
   "outputs": [],
   "source": [
    "print(\"Overall: s: \",np.sum(y==1),\", b: \",np.sum(y==-1),\" ,total:\",len(y))\n",
    "print(\"NoErrors: s: \",np.sum(y_c==1),\", b: \",np.sum(y_c==-1),\" ,total:\",len(y_c))\n",
    "for n in range(tX_c.shape[1]):\n",
    "    plt.figure(figsize=(20,4))\n",
    "    plt.subplot(131)\n",
    "    plt.hist([tX_c[y_c==1,n],tX_c[y_c==-1,n]], 20, density=True, histtype='bar', stacked=True)\n",
    "    plt.legend(['s','b'])\n",
    "    plt.title('Feature '+str(n))\n",
    "    plt.subplot(132)\n",
    "    plt.title('s histogram feature '+str(n))\n",
    "    plt.hist(tX_c[y_c==1,n], 20, density=True, histtype='bar', stacked=True)\n",
    "    plt.subplot(133)\n",
    "    plt.title('b histogram feature '+str(n))\n",
    "    plt.hist(tX_c[y_c==-1,n], 20, density=True, histtype='bar', stacked=True)    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove features with error values\n",
    "idx_gf = np.arange(tX.shape[1])[np.all(tX!=-999, axis=0)]\n",
    "y_gf = y\n",
    "tX_gf = tX[:,idx_gf]\n",
    "# regularize\n",
    "mean = np.mean(tX_gf, axis=0)\n",
    "std = np.std(tX_gf, axis=0)\n",
    "tX_gf = (tX_gf-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Overall: s: \",np.sum(y==1),\", b: \",np.sum(y==-1),\" ,total:\",len(y))\n",
    "for n in range(tX_gf.shape[1]):\n",
    "    plt.figure(figsize=(20,4))\n",
    "    plt.subplot(131)\n",
    "    plt.hist([tX_gf[y_gf==1,n],tX_gf[y_gf==-1,n]], 20, density=True, histtype='bar', stacked=True)\n",
    "    plt.legend(['s','b'])\n",
    "    plt.title('Feature '+str(idx_gf[n]))\n",
    "    plt.subplot(132)\n",
    "    plt.title('s histogram feature '+str(idx_gf[n]))\n",
    "    plt.hist(tX_gf[y_gf==1,n], 20, density=True, histtype='bar', stacked=True)\n",
    "    plt.subplot(133)\n",
    "    plt.title('b histogram feature '+str(idx_gf[n]))\n",
    "    plt.hist(tX_gf[y_gf==-1,n], 20, density=True, histtype='bar', stacked=True)    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5iFp9zV4pjg"
   },
   "source": [
    "# Actual predictions start from here\n",
    "\n",
    "After having looked at the data we will now do some actual predictions using different models andd parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TEST_PATH = '../data/test.csv.zip' \n",
    "#test_y, test_X, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ql2-oUuA4ynz"
   },
   "source": [
    "## Generate predictions using only features with no errrors throughought\n",
    "\n",
    "This enables us to use some of the methods from the course directly, without having to adjust some of the functionnality to account for the fact that a lot of errors are in the dataset. First let us see which features from the test dataset are error free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import test data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "test_x_df = pd.read_csv(DATA_TEST_PATH, index_col='Id').drop('Prediction', axis='columns')\n",
    "test_x_df\n",
    "\n",
    "train_x_df = pd.read_csv(DATA_TRAIN_PATH, index_col='Id').drop('Prediction', axis='columns')\n",
    "train_y_df = pd.read_csv(DATA_TRAIN_PATH, index_col='Id', usecols=['Id', 'Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns with -999 errors\n",
    "error_cols_te = np.full(test_x_df.shape[1], False)\n",
    "for i in range(test_x_df.shape[1]):\n",
    "#     print(test_x_df.iloc[:,i].values)\n",
    "#     print(-999.0 in test_x_df.iloc[:,i].values)\n",
    "    if -999.0 in test_x_df.iloc[:,i].values:\n",
    "        error_cols_te[i] = True\n",
    "print(f\"There are {error_cols_te.tolist().count(True)} test error columns are which are : \\n{error_cols_te}\\n\")\n",
    "\n",
    "error_cols_tr = np.full(train_x_df.shape[1], False)\n",
    "for i in range(train_x_df.shape[1]):\n",
    "    if -999.0 in train_x_df.iloc[:,i].values:\n",
    "        error_cols_tr[i] = True\n",
    "print(f\"There are {error_cols_tr.tolist().count(True)} train error columns are which are : \\n{error_cols_tr}\")\n",
    "\n",
    "no_error_cols_tr_te = (~(error_cols_tr | error_cols_te)).tolist()\n",
    "print(f\"They have {no_error_cols_tr_te.count(True)} following shared non error columns :\\n{no_error_cols_tr_te}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this figured out we can now extract the valid columns from test and train data, do some training and testing on data, then generate answers for the test data and submit to aicrowd !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkMBGpay4pjh"
   },
   "source": [
    "# Save prediction ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i--OcRst4pjh"
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grQDhIJ84pjk"
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "project1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
